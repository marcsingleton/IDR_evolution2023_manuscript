\begin{center}
\textbf{Listing B1: Implementation of Baum-Welch for categorical distributions}
\end{center}

\begin{NotebookIn}
epsilon = 0.01
maxiter = 100

ll0 = None
model_hat = HMM(t_dists=t_dists_hat,
                e_dists=e_dists_hat,
                start_dist=start_dist_hat)
for numiter in range(maxiter):
    # Initialize count dictionaries
    ps = []
    t_counts = {state1: {state2: 0 for state2 in t_dist}
                for state1, t_dist in t_dists_hat.items()}
    e_counts = {state: {emit: 0 for emit in e_dist}
                for state, e_dist in e_dists_hat.items()}
    start_count = {state: 0 for state in start_dist_hat}

    # Get counts across all examples
    for example in data:
        xs, ys = example
        fs, ss_f = model_hat.forward(ys)
        bs, ss_b = model_hat.backward(ys)

        p = reduce(lambda x, y: x+y, map(log, ss_f))
        ss_f = list(accumulate(map(log, ss_f)))
        ss_b = list(accumulate(map(log, ss_b[::-1])))[::-1]
        ps.append(p)

        # t_counts
        for t in range(len(ys)-1):
            for state1, t_count in t_counts.items():
                for state2 in t_count:
                    term1 = fs[state1][t] * t_dists_hat[state1][state2]
                    term2 = e_dists_hat[state2][ys[t+1]] * bs[state2][t+1]
                    count = term1 * term2
                    t_count[state2] += count * exp(ss_f[t] + ss_b[t+1] - p)

        # e_counts
        for t in range(len(ys)):
            for state, e_count in e_counts.items():
                if ys[t] in e_count:
                    count = fs[state][t]*bs[state][t]
                    e_count[ys[t]] += count * exp(ss_f[t] + ss_b[t] - p)

        # start_count
        for state in start_count:
            count = fs[state][0]*bs[state][0]
            start_count[state] += count * exp(ss_f[0] + ss_b[0] - p)

    # Format parameters for display
    t_string = pprint.pformat(t_dists_hat)
    t_string = t_string.replace('\n', '\n' + len('t_dists: ')*' ')
    e_string = pprint.pformat(e_dists_hat)
    e_string = e_string.replace('\n', '\n' + len('e_dists: ')*' ')
    start_string = pprint.pformat(start_dist_hat)

    # Check stop condition
    # Don't want to repeat calculations,
    # so ith iterate checks previous update
    # For example, 0th iterate shows initial parameters,
    # and 1st iterate shows results of first update
    ll = sum(ps)
    if ll0 is not None and abs(ll - ll0) < epsilon:
        print(f'FINAL VALUES')
        print('log-likelihood:', ll)
        print('delta log-likelihood:', ll-ll0 if ll0 is not None else None)
        print('t_dists:', t_string)
        print('e_dists:', e_string)
        print('start_dist:', start_string)
        break

    # Print results
    print(f'ITERATION {numiter}')
    print('log-likelihood:', ll)
    print('delta log-likelihood:', ll-ll0 if ll0 is not None else None)
    print('t_dists:', t_string)
    print('e_dists:', e_string)
    print('start_dist:', start_string)
    print()

    # Normalize all counts and update model
    t_dists_hat = {}
    for state1, t_count in t_counts.items():
        t_sum = sum(t_count.values())
        t_dist_hat = {}
        for state2, count in t_count.items():
            t_dist_hat[state2] = count / t_sum
        t_dists_hat[state1] = t_dist_hat

    e_dists_hat = {}
    for state, e_count in e_counts.items():
        e_sum = sum(e_count.values())
        e_dist_hat = {}
        for emit, count in e_count.items():
            e_dist_hat[emit] = count / e_sum
        e_dists_hat[state] = e_dist_hat

    start_sum = sum(start_count.values())
    start_dist_hat = {}
    for state, count in start_count.items():
        start_dist_hat[state] = count / start_sum

    ll0 = ll
    model_hat = HMM(t_dists=t_dists_hat,
                    e_dists=e_dists_hat,
                    start_dist=start_dist_hat)
\end{NotebookIn}